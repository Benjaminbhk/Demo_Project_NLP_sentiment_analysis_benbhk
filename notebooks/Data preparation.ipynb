{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f11114e-8572-410d-9a1f-67c577312436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c936da9c-9253-4aa8-b1b6-db4431557000",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571cfe88-7d60-493a-8d30-929703317e01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data preparation (from row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76c39de-5456-4ee2-9f7f-6a4168194a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_df = pd.read_csv('../raw_data/Corona_NLP_test.csv')\n",
    "Train_df = pd.read_csv('../raw_data/Corona_NLP_train.csv',encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a8784d-bf5d-4a6c-9ded-a0ce1f68a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_df.drop(columns=['UserName','ScreenName','Location','TweetAt'],inplace=True)\n",
    "Train_df.drop(columns=['UserName','ScreenName','Location','TweetAt'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1412abb-ee45-4aef-98c9-63e59b83a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_df.to_csv('../Demo_Project_NLP_sentiment_analysis_benbhk/data/X_test.csv')\n",
    "Train_df.to_csv('../Demo_Project_NLP_sentiment_analysis_benbhk/data/X_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f442fa1-b374-46ad-8ce1-48d494a45102",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data loading (from data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d393bebf-191c-4f33-bdaf-3bfc741e9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_df = pd.read_csv('../Demo_Project_NLP_sentiment_analysis_benbhk/data/X_test.csv')\n",
    "Train_df = pd.read_csv('../Demo_Project_NLP_sentiment_analysis_benbhk/data/X_train.csv')\n",
    "Test_df.dropna(inplace=True)\n",
    "Train_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65eaa622-253a-4d49-b59f-0e9f18795e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "Train_df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d9a443-8218-4b0e-af40-406d1c7dcb97",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdf08d97-5a0e-40b0-9a70-6fb0e33f718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_df['Sentiment_num'] = -1\n",
    "Test_df.loc[Test_df['Sentiment'] =='Extremely Negative','Sentiment_num'] = 0\n",
    "Test_df.loc[Test_df['Sentiment'] =='Negative','Sentiment_num'] = 1\n",
    "Test_df.loc[Test_df['Sentiment'] =='Neutral','Sentiment_num'] = 2\n",
    "Test_df.loc[Test_df['Sentiment'] =='Positive','Sentiment_num'] = 3\n",
    "Test_df.loc[Test_df['Sentiment'] =='Extremely Positive','Sentiment_num'] = 4\n",
    "\n",
    "Train_df.loc[Train_df['Sentiment'] =='Extremely Negative','Sentiment_num'] = 0\n",
    "Train_df.loc[Train_df['Sentiment'] =='Negative','Sentiment_num'] = 1\n",
    "Train_df.loc[Train_df['Sentiment'] =='Neutral','Sentiment_num'] = 2\n",
    "Train_df.loc[Train_df['Sentiment'] =='Positive','Sentiment_num'] = 3\n",
    "Train_df.loc[Train_df['Sentiment'] =='Extremely Positive','Sentiment_num'] = 4\n",
    "\n",
    "# Test_df[Test_df['Sentiment']=='Extremely Negative']['Sentiment_num'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "483784ff-c592-472f-b102-a56a222b3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = Test_df['OriginalTweet'].to_numpy()\n",
    "X_train = Train_df['OriginalTweet'].to_numpy()\n",
    "y_test = Test_df['Sentiment_num'].to_numpy(dtype=int)\n",
    "y_train = Train_df['Sentiment_num'].to_numpy(dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "263cb78e-625d-4444-9be0-efdbc07f7626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3798,) (3798,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7cb1121-cea5-458d-be6d-b2c6b949d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41155,) (41155,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de88783a-6213-4715-8770-40478406b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    X_train[i] = str.encode(X_train[i])\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = str.encode(X_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "740ead14-29de-44e6-84c7-2c45d0b111dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 17:57:52.090506: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-21 17:57:52.090568: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in X_train]\n",
    "X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88f2ae5c-4f1d-45c9-a205-ad18562fe9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e958433-f5fc-4c34-bbaf-539a21cd04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd0b13-471f-4b4b-bc7f-53c3424f5f86",
   "metadata": {},
   "source": [
    "## Model cr√©ation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44dc8b51-5ded-42c8-a011-804b87d82b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 17:56:10.013123: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-21 17:56:10.013202: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-21 17:56:10.013235: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (BenLaptop-V8N6C5JR): /proc/driver/nvidia/version does not exist\n",
      "2022-04-21 17:56:10.013759: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Flatten\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Masking(mask_value=0, input_shape=(200,100)))\n",
    "model.add(layers.LSTM(units=20, activation=\"tanh\"))\n",
    "model.add(layers.Dense(10,activation=\"relu\"))\n",
    "model.add(layers.Dense(5,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe58163b-4233-4b44-934a-64c50fb662ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_pad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4874/4006224806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m history = model.fit(X_train_pad, y_train,\n\u001b[0m\u001b[1;32m     10\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_pad' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(patience=10,restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_pad, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=20, \n",
    "                    callbacks=[es],\n",
    "                    batch_size=32, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459a56a-e1d3-4b95-afb2-bc6843720ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00042cba-b194-4ccf-972f-eb27fa1f9a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b6d31-1b01-482a-9a8c-05921785bf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35fb2d-2323-4734-b597-4761b1f4480e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963707c6-35af-4a3b-bedb-ea1197c12268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a989c2-e65e-482e-8d94-68c16d8c6b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb214e0-3512-4d92-b623-9bddeec49b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "131bfce1-db03-4cfc-b233-b7a128cdafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_transfer = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a2cc1b1-c5ea-42bb-a353-347570ca5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670df24a-cd28-46c9-b7e9-e0734c20c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593b277-9e7e-4314-998d-002e244fc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "# from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Flatten\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Masking(mask_value=0, input_shape=(200,50)))\n",
    "model.add(layers.LSTM(units=20, activation=\"tanh\"))\n",
    "model.add(layers.Dense(10,activation=\"relu\"))\n",
    "model.add(layers.Dense(1,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "599ed93d-9eb2-466e-9808-e73a8b5e553a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4874/2798791029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model.compile(loss='binary_crossentropy',\n\u001b[0m\u001b[1;32m      4\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               metrics=['accuracy'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(patience=10,restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_pad_2, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=500, \n",
    "                    callbacks=[es],\n",
    "                    batch_size=32, \n",
    "                    verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
