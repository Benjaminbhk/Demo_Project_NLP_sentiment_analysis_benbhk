{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4bd9ee6-6ae9-4442-a762-cf6401ae0bf4",
   "metadata": {},
   "source": [
    "# Model Transformer (with word2vec transfert learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b675d-0098-4dd6-9395-c72091e4b6dd",
   "metadata": {},
   "source": [
    "## Data loading (from data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063b916a-150e-40a2-83ba-5afceffee93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Test_df = pd.read_csv('../Demo_Project_NLP_sentiment_analysis_benbhk/data/X_test.csv')\n",
    "Train_df = pd.read_csv('../Demo_Project_NLP_sentiment_analysis_benbhk/data/X_train.csv')\n",
    "Test_df.dropna(inplace=True)\n",
    "Train_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4961f23f-214e-43aa-9b9e-05b730421d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "Train_df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6a1b7-894a-4e4e-b01c-3f4f170ebaf1",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1b9626-9a01-401d-8151-55d356bf4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_df['Sentiment_num'] = -1\n",
    "Test_df.loc[Test_df['Sentiment'] =='Extremely Negative','Sentiment_num'] = 0\n",
    "Test_df.loc[Test_df['Sentiment'] =='Negative','Sentiment_num'] = 1\n",
    "Test_df.loc[Test_df['Sentiment'] =='Neutral','Sentiment_num'] = 2\n",
    "Test_df.loc[Test_df['Sentiment'] =='Positive','Sentiment_num'] = 3\n",
    "Test_df.loc[Test_df['Sentiment'] =='Extremely Positive','Sentiment_num'] = 4\n",
    "\n",
    "Train_df.loc[Train_df['Sentiment'] =='Extremely Negative','Sentiment_num'] = 0\n",
    "Train_df.loc[Train_df['Sentiment'] =='Negative','Sentiment_num'] = 1\n",
    "Train_df.loc[Train_df['Sentiment'] =='Neutral','Sentiment_num'] = 2\n",
    "Train_df.loc[Train_df['Sentiment'] =='Positive','Sentiment_num'] = 3\n",
    "Train_df.loc[Train_df['Sentiment'] =='Extremely Positive','Sentiment_num'] = 4\n",
    "\n",
    "# Test_df[Test_df['Sentiment']=='Extremely Negative']['Sentiment_num'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc34c63-2e95-492d-b8e1-2cde71d2571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = Test_df['OriginalTweet'].to_numpy()\n",
    "X_train = Train_df['OriginalTweet'].to_numpy()\n",
    "y_test = Test_df['Sentiment_num'].to_numpy(dtype=int)\n",
    "y_train = Train_df['Sentiment_num'].to_numpy(dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdd3067e-00bf-4022-9b8b-adec1543fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    X_train[i] = str.encode(X_train[i])\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = str.encode(X_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920a9a01-ff63-4515-bf10-8ef9cbac6206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 13:57:40.965594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-26 13:57:40.965783: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in X_train]\n",
    "X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0cff37-bc7f-4086-8a1e-10dd85845541",
   "metadata": {},
   "source": [
    "## Word2vec loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "142cf1d6-2dc5-4945-b46b-09fd9ca3f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6204a4b0-cd59-42ff-83b2-2fdd300292cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 357 ms, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2vec_transfer = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a4928f-b93a-4e64-acc3-b2ae6871bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3651dc9-0349-4743-a178-9e533f85e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_transfer.save_word2vec_format('../Demo_Project_NLP_sentiment_analysis_benbhk/models/glove-wiki-gigaword-50.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9039d577-619f-43b1-8f08-88733f8e81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# vectors_reloaded = KeyedVectors.load_word2vec_format('../Demo_Project_NLP_sentiment_analysis_benbhk/models/glove-wiki-gigaword-50.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7424b2-feb9-44c4-a5f6-b8889013fe04",
   "metadata": {},
   "source": [
    "## Word2vec processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d52808a7-9c41-4819-affb-cc25817fefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)\n",
    "\n",
    "# X_train_embed_2 = embedding(vectors_reloaded, X_train)\n",
    "# X_test_embed_2 = embedding(vectors_reloaded, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f8db1-4f6d-425b-bd38-2e8929a136fa",
   "metadata": {},
   "source": [
    "## Padding (post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6559daa1-2d33-4a52-9c89-5ad39eacdcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=100)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fde092-3bda-4acd-8445-37953824045e",
   "metadata": {},
   "source": [
    "## Model creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "821d0ea8-5852-4a31-bf83-1f85b85a6ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b45f2dd-537a-4cc4-8413-712019dcd098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48103a13-6639-4a30-9f0c-06132d1401a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e754663-ec22-4ca8-bf4e-b9f86ea09893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 13:59:40.225491: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-26 13:59:40.226019: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-26 13:59:40.227457: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (BenLaptop-V8N6C5JR): /proc/driver/nvidia/version does not exist\n",
      "2022-04-26 13:59:40.231350: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 100  # Embedding size for each token\n",
    "num_heads = 3  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "maxlen = 100\n",
    "# vocab_size = 200\n",
    "\n",
    "inputs = Input(shape=(maxlen,embed_dim))\n",
    "# embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "# x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(inputs)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdf60e-2b24-4145-94e6-b9f9d0e521c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 14:01:57.728668: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1316960000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 343/1029 [=========>....................] - ETA: 5:26 - loss: 1.4116 - accuracy: 0.3612"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "es = EarlyStopping(patience=10,restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_pad_2, y_train, \n",
    "                    batch_size=32, epochs=500,\n",
    "                    callbacks=[es], \n",
    "                    # validation_data=(X_test_pad_2), y_test\n",
    "                    validation_split=0.2\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b6485d-f9e3-4192-88df-6f1a8310949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../Demo_Project_NLP_sentiment_analysis_benbhk/sformer_weights_W2V_50_3attention_head_accuracy_.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7042e71e-8b0f-4522-bd7e-b2073e9ac35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 5s 42ms/step - loss: 1.0297 - accuracy: 0.5732\n",
      "loss: 1.030\n",
      "accuracy: 0.573\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test_pad_2, y_test, verbose=1)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    \n",
    "    print(\"%s: %.3f\" % (name, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
